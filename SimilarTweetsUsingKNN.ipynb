{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e31273b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Vidit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import string\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords, twitter_samples\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from functions import process_tweet,cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa0d580f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Vidit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1459ace3",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "all_negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "tweets = all_positive_tweets + all_negative_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1efff992",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_embeddings = pickle.load(open(\"en_embeddings.p\", \"rb\"))\n",
    "# fr_embeddings_subset = pickle.load(open(\"fr_embeddings.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46868621",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweet_embedding(tweet,en_embeddings):\n",
    "    t_embedding = np.zeros(300)\n",
    "    cleaned_tweet = process_tweet(tweet)\n",
    "    for word in cleaned_tweet:\n",
    "        t_embedding += en_embeddings.get(word,0)\n",
    "    return t_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6282b3fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00268555, -0.15378189, -0.55761719, -0.07216644, -0.32263184])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_tweet = \"RT @Twitter @chapagain Hello There! Have a great day. :) #good #morning http://chapagain.com.np\"\n",
    "\n",
    "tweet_embedding = get_tweet_embedding(custom_tweet, en_embeddings)\n",
    "tweet_embedding[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84129b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_document_vecs(tweets,en_embedding):\n",
    "    l = []\n",
    "    d = {}\n",
    "    for i,tweet in enumerate(tweets):\n",
    "        d[i] = get_tweet_embedding(tweet,en_embedding)\n",
    "        l.append(get_tweet_embedding(tweet,en_embedding))\n",
    "    document_matrix = np.vstack(l)\n",
    "    return document_matrix,d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d2fece4",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_vecs, idx_tweet = get_document_vecs(tweets, en_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "614d478b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dictionary 10000\n",
      "Shape of document_vecs (10000, 300)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Length of dictionary {len(idx_tweet)}\")\n",
    "print(f\"Shape of document_vecs {document_vecs.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bce3ce2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Vidit\\NLP\\functions.py:62: RuntimeWarning: invalid value encountered in true_divide\n",
      "  cos = dot / (norma * normb)\n"
     ]
    }
   ],
   "source": [
    "my_tweet = 'i am happy'\n",
    "process_tweet(my_tweet)\n",
    "tweet_embedding = get_tweet_embedding(my_tweet, en_embeddings)\n",
    "\n",
    "idx = np.argmax(cosine_similarity(document_vecs, tweet_embedding.reshape(300,1)))\n",
    "print(tweets[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "10611fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Searching Closest Tweet using Locality Sensitive Hashsing\n",
    "# Much faster than searching through all 10000 tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "95b871c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of vectors is 10000 and each has 300 dimensions.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of vectors is {len(tweets)} and each has {len(idx_tweet[1])} dimensions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f4af9b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each plane divides our space into 2 parts\n",
    "# n-planes divides into 2^n parts/buckets\n",
    "# We have 10000 vectors we want to divide them such that each bucket has 16 vectors\n",
    "# Therefore 10000/16 = 625 buckets\n",
    "# Number of planes is log2(625) which is close to 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a790b871",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_planes = 10\n",
    "n_dim = 300\n",
    "# Number of times to repeat the hashing to improve the search.\n",
    "n_universes = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "586ecd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "planes_l = [np.random.normal(size=(n_dim, n_planes)) for _ in range(n_universes)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d4ed0dd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 10)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "planes_l[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ca720702",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_value_of_vector(v, planes):\n",
    "    sign_of_dot_product = np.sign(np.dot(v,planes))\n",
    "    \n",
    "    # set h to be false (eqivalent to 0 when used in operations) if the sign is negative,\n",
    "    # and true (equivalent to 1) if the sign is positive (1,10) shaped vector\n",
    "    h = sign_of_dot_product>=0 \n",
    "    h = np.squeeze(h)\n",
    "    \n",
    "    hash_value = 0\n",
    "    n_planes = planes.shape[1] # 10\n",
    "    \n",
    "    for i in range(n_planes):\n",
    "        hash_value += np.power(2,i)*h[i]\n",
    "\n",
    "    hash_value = int(hash_value)\n",
    "    return hash_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b7fa5644",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_hash_table(vecs, planes): # this is for a single set of 10 planes\n",
    "    num_of_planes = planes.shape[1] # 10\n",
    "    num_buckets = 2**num_of_planes\n",
    "\n",
    "    hash_table = {i:[] for i in range(num_buckets)}\n",
    "    id_table = {i:[] for i in range(num_buckets)}\n",
    "\n",
    "    for i, v in enumerate(vecs):\n",
    "        h = hash_value_of_vector(v,planes)\n",
    "        hash_table[h].append(v)\n",
    "        id_table[h].append(i)\n",
    "\n",
    "    return hash_table, id_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aa748daf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on 1 set\n",
      "Working on 2 set\n",
      "Working on 3 set\n",
      "Working on 4 set\n",
      "Working on 5 set\n",
      "Working on 6 set\n",
      "Working on 7 set\n",
      "Working on 8 set\n",
      "Working on 9 set\n",
      "Working on 10 set\n",
      "Working on 11 set\n",
      "Working on 12 set\n",
      "Working on 13 set\n",
      "Working on 14 set\n",
      "Working on 15 set\n",
      "Working on 16 set\n",
      "Working on 17 set\n",
      "Working on 18 set\n",
      "Working on 19 set\n",
      "Working on 20 set\n",
      "Working on 21 set\n",
      "Working on 22 set\n",
      "Working on 23 set\n",
      "Working on 24 set\n",
      "Working on 25 set\n"
     ]
    }
   ],
   "source": [
    "hash_tables = []\n",
    "id_tables = []\n",
    "for i in range(n_universes):  # there are 25 sets of 10 planes\n",
    "    print(f\"Working on {i+1} set\")\n",
    "    planes = planes_l[i]\n",
    "    hash_table, id_table = make_hash_table(document_vecs, planes)\n",
    "    hash_tables.append(hash_table)\n",
    "    id_tables.append(id_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d904b4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nearest_neighbor(v, candidates, k=1):\n",
    "    similarity_l = []\n",
    "    for row in candidates:\n",
    "        cos_similarity = cosine_similarity(v,row)\n",
    "        similarity_l.append(cos_similarity)\n",
    "        \n",
    "    sorted_ids = np.argsort(similarity_l)\n",
    "    k_idx = sorted_ids[-k:]\n",
    "    return k_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "30b16440",
   "metadata": {},
   "outputs": [],
   "source": [
    "def closest(tweet_id,tweets,en_embedding,planes,hash_tables,id_tables,k):\n",
    "    tweet = tweets[tweet_id]\n",
    "    doc_em = get_tweet_embedding(tweet,en_embedding)\n",
    "    id_to_consider = []\n",
    "    vecs_to_consider = []\n",
    "    \n",
    "    for i in range(len(planes)):\n",
    "        h_v = hash_value_of_vector(doc_em,planes[i])\n",
    "        hash_table = hash_tables[i] # hash table for corresponding set of planes\n",
    "        doc_vecs = hash_table[h_v]\n",
    "        id_table = id_tables[i]\n",
    "        id_vecs = id_table[h_v]\n",
    "        \n",
    "        if tweet_id in id_vecs:\n",
    "            id_vecs.remove(tweet_id)\n",
    "        \n",
    "        for i,new_id in enumerate(id_vecs):\n",
    "            if new_id not in id_to_consider:\n",
    "                id_to_consider.append(new_id)\n",
    "                vecs_to_consider.append(tweets[new_id])\n",
    "                \n",
    "    vecs_to_consider_arr = np.array(vecs_to_consider)\n",
    "    li,a = get_document_vecs(vecs_to_consider_arr,en_embedding)\n",
    "    tweet_vec = get_tweet_embedding(tweet,en_embedding)\n",
    "    idx = nearest_neighbor(tweet_vec,li.tolist(),k)\n",
    "    final_ids = [id_to_consider[i] for i in idx]\n",
    "    return final_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6a459307",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3128, 25, 9770]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = closest(888,tweets,en_embeddings,planes_l,hash_tables,id_tables,3)\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6db7f053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Tweet:  Thanks for updating your profile page @AlexaPoppe :-)  http://t.co/JK3NSXIRMe\n",
      "\n",
      "Similar Tweets:\n",
      "1)  @triangledarren thank you :)\n",
      "2)  @Bosslogic @amellywood @CW_Arrow @ARROWwriters Thank you! :-)\n",
      "3)  @The5BallOver @Radio702 :-( It's not a challenge though. Please check our FB page for entries and rather do a substitution. Thanks!\n"
     ]
    }
   ],
   "source": [
    "print(\"Original Tweet: \",tweets[888])\n",
    "print()\n",
    "print(\"Similar Tweets:\")\n",
    "print(\"1) \",tweets[ids[0]])\n",
    "print(\"2) \",tweets[ids[1]])\n",
    "print(\"3) \",tweets[ids[2]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
